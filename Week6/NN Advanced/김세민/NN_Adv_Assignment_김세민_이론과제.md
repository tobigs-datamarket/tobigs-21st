# Learning Rate Scheduler 소개

> **Step Decay**: 일정한 epoch마다 learning rate를 감소시키는 방법이다. 초기 learning rate를 설정하고, 특정 epoch 혹은 특정 조건이 충족되면 learning rate를 감소시킨다. 이 방법은 처음에는 빠르게 학습하다가 최적점에 가까워질수록 세밀한 조정이 필요한 경우에 유용하다. 예를 들어, 초기 learning rate를 0.01로 설정하고, 매 10 epoch마다 learning rate를 0.1씩 감소시키는 것이 가능하다.

> **Exponential Decay**: 매 epoch마다 learning rate를 감소시키는데, 감소율이 지수적으로 감소한다. 이 방법은 학습 초기에는 빠르게 수렴하면서, 최적점에 가까워질수록 학습률을 점진적으로 줄여 미세 조정을 돕는다. 예를 들어, 초기 learning rate를 0.1로 설정하고, 매 epoch마다 0.9를 곱하여 감소시키는 것이 가능하다.

> **Cosine Annealing**: 주기적으로 learning rate를 감소시키는데, 코사인 함수를 이용하여 learning rate를 조절한다. 이는 학습 초기에는 빠르게 수렴하면서, 지역 최적점에 빠지지 않고 전역 최적점을 탐색할 수 있도록 도와준다. 예를 들어, 학습의 첫 번째 주기에서는 learning rate를 빠르게 줄이고, 두 번째 주기에서는 learning rate를 더욱 천천히 감소시킨다.

> **Cyclic Learning Rates**: 주기적으로 learning rate를 변경하여 학습 속도를 조절한다. 주기마다 learning rate를 크게 조절하며, 이를 통해 지역 최적점을 빠져나와 전역 최적점을 탐색할 수 있다. 이 방법은 모델이 다양한 learning rate를 경험하도록 돕고, 학습 과정을 더욱 다이나믹하게 만든다.



# Training Error와 Generalization Error 사이 간극을 줄이는 방법론

Training Error와 Generalization Error 사이의 간극을 줄이는 방법에는 다양한 것들이 존재한다. 

우선 Error 값은 모델의 복잡도와 밀접한 연관이 있다. 일반적으로 모델의 복잡도가 높아질수록 과적합 발생 가능성이 현저히 증가하게 되고, 일반화 성능의 저해를 야기할 수 있다. 때문에 너무 복잡하게 짜여진 모델은 외려 전반적인 성능을 떨어뜨릴 수 있다. 때문에 적절한 파라미터 수와 구조를 가지도록 모델을 재구성하는 게 좋은 방법이 될 수 있다. 

두 번째는 Early Stopping의 사용이다. Early Stopping은 훈련 중에 검증 데이터에 대한 성능이 떨어지기 시작하면 조기에 훈련을 종료하는 방법이다. 가장 흔하게 사용되는 방식은 사전에 정의된 patience steps 내로 검증 데이터에 대한 성능이 개선되지 않을 경우, 훈련을 종료시키는 방식이다. 이는 모델이 성능 개선 없이 훈련 데이터에만 과적합되는 일을 조기에 방지할 수 있기 때문에 성능 향상에 기여할 수 있다. 

세 번째 방법은 cross-validation이다. 모델의 일반화 성능을 더욱 정확하게 평가하기 위해서는 고정된 검증 데이터만으로 부족할 때가 많이 있다. 이 경우, 데이터를 여러 부분 집합으로 나눈 뒤 검증 데이터셋을 바꿔가며 검증 작업을 수행하면 일반화 성능 평가가 더욱 정확하게 이루어지게 된다. 따라서 cross-validation을 통해 모델의 학습 과정을 모니터링하는 방식도 모델 성능 개선에 기여할 수 있다. 

마지막 방식은 바로 앙상블 기법이다. 여러 모델을 결합하여 예측을 생성하면 한 모델에만 치우치지 않은 균형 잡힌 결과가 도출된다. 이를 통해 더욱 견고한 예측 결과를 얻을 수 있다.
